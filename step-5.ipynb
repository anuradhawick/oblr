{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e15f4ec",
   "metadata": {},
   "source": [
    "# GNN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fb424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch_cluster import random_walk\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d33c7f",
   "metadata": {},
   "source": [
    "### experiment path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = './test_data/'\n",
    "\n",
    "truth = np.array(open(exp + \"ground_truth.txt\").read().strip().split(\"\\n\"))\n",
    "data = np.load(exp + 'data.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0079366",
   "metadata": {},
   "source": [
    "### fetch data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccd888",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = data['edges']\n",
    "comp = data['scaled']\n",
    "read_cluster = data['read_cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e049d3",
   "metadata": {},
   "source": [
    "# GraphSAGE Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers, device):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        hidden_channels = (in_channels + out_channels)//2\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.fc2 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            x = F.relu(x)                \n",
    "            x = F.dropout(x, p=0.2, training=self.training)\n",
    "            \n",
    "        x = self.fc1(x)\n",
    "        embedding = x\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "    \n",
    "        return x.log_softmax(dim=-1), embedding\n",
    "\n",
    "    def inference(self, x_all, subgraph_loader):   \n",
    "        idx = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                if i==0:\n",
    "                    idx += list(n_id[:batch_size].numpy())\n",
    "                edge_index, _, size = adj.to(self.device)\n",
    "                x = x_all[n_id].to(self.device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = self.convs[i]((x, x_target), edge_index)\n",
    "                x = F.relu(x)\n",
    "                xs.append(x)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "        \n",
    "        x = self.fc1(x_all)\n",
    "        x = F.relu(x)        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = x.cpu()\n",
    "\n",
    "        return np.array(idx), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out, embd = model(x[n_id], adjs)\n",
    "        \n",
    "        loss = F.nll_loss(out, y[n_id[:batch_size]])\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += float(loss)\n",
    "\n",
    "    loss = total_loss / len(train_loader)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, x, subgraph_loader):\n",
    "    model.eval()\n",
    "\n",
    "    out = model.inference(x, subgraph_loader)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_data(features, edges):\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "    data = Data(x=torch.tensor(features).float(), edge_index=edge_index.t().contiguous())\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = get_graph_data(comp, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(features, read_cluster):\n",
    "    train_idx = read_cluster.T[0]\n",
    "    train_idx = torch.LongTensor(train_idx)\n",
    "\n",
    "    y = -1 * torch.ones(len(features), dtype=torch.long)\n",
    "    y[train_idx] = torch.LongTensor(read_cluster.T[1])\n",
    "    no_classes = len(set(read_cluster.T[1]))\n",
    "    \n",
    "    return train_idx, y, no_classes\n",
    "\n",
    "train_idx, y, no_classes = get_train_data(comp, read_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d453f8",
   "metadata": {},
   "source": [
    "### prepping models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SAGE(data.x.shape[1], no_classes, 2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a240fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using the device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e843bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model summary\\n-------------\\n\", model, \"\\n-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997cdb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = NeighborSampler(data.edge_index, \n",
    "                               node_idx=train_idx,\n",
    "                               sizes=[50, 50], \n",
    "                               batch_size=64,\n",
    "                               pin_memory=True,\n",
    "                               shuffle=True, \n",
    "                               drop_last=True, \n",
    "                               num_workers=8)\n",
    "\n",
    "subgraph_loader = NeighborSampler(data.edge_index, \n",
    "                                  node_idx=None, \n",
    "                                  sizes=[-1],\n",
    "                                  pin_memory=True,\n",
    "                                  batch_size=10240, \n",
    "                                  shuffle=False,\n",
    "                                  num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c76dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.x.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b817708",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=10e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6ae2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# for plotting\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(-1, 201)\n",
    "ax.set_ylim(0, 1)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "losses = []\n",
    "prev_loss = 100\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss = train(model, x, y, optimizer, train_loader, device)\n",
    "    dloss = prev_loss-loss\n",
    "    prev_loss = loss\n",
    "    # plot params\n",
    "    losses.append(loss)    \n",
    "    ax.clear()\n",
    "    ax.plot(losses)\n",
    "    ax.set_xlim(-1, 101)\n",
    "    ax.set_ylim(0, 1)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}', end=\"\\r\", flush=True)\n",
    "    \n",
    "    if loss < 0.05:\n",
    "        print()\n",
    "        print('Early stopping, loss less than 0.05')\n",
    "        break\n",
    "    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca07b080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we must keep track of classified ids, since lonely nodes are not classified\n",
    "idx, preds = test(model, x, subgraph_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a5210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = torch.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35016a6",
   "metadata": {},
   "source": [
    "### final result to be carried forward\n",
    "\n",
    "This consits of \n",
    "\n",
    "* **classified:** Index of the read (0 to N-1, where N is number of reads)\n",
    "* **classes:** Class/bin of the read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(exp +'classes.npz', classes=classes.numpy(), classified=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb048c0",
   "metadata": {},
   "source": [
    "### Evaluation if truth available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = np.load(exp + 'classes.npz')\n",
    "\n",
    "classes = classification['classes']\n",
    "idx = classification['classified']\n",
    "no_classes = len(set(classes))\n",
    "spec_set = set(truth) - {'Unknown'}\n",
    "\n",
    "\n",
    "matrix = np.zeros((len(spec_set), no_classes))\n",
    "spec_idx = {s:n for n, s in enumerate(spec_set)}\n",
    "idx_spec = {n:s for n, s in enumerate(spec_set)}\n",
    "\n",
    "for n, (c, t) in tqdm(enumerate(zip(classes, truth[idx]))):\n",
    "    if t == 'Unknown':\n",
    "        continue\n",
    "    matrix[spec_idx[t], c] += 1\n",
    "    \n",
    "tot = matrix.sum()\n",
    "row_sum = matrix.max(0).sum()\n",
    "col_sum = matrix.max(1).sum()\n",
    "\n",
    "print(matrix.shape)\n",
    "p, r = 100*row_sum/tot, 100*col_sum/tot\n",
    "f1 = 2 * p * r / (p + r)\n",
    "\n",
    "print(f'Precision  =  {p:3.2f}')\n",
    "print(f'Recall     =  {r:3.2f}')\n",
    "print(f'F1-score   =  {f1:3.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d083292",
   "metadata": {},
   "source": [
    "### separate reads for binning purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(exp + 'binned_reads'):\n",
    "    shutil.rmtree(exp + 'binned_reads')\n",
    "os.mkdir(exp + 'binned_reads')\n",
    "\n",
    "bin_file = {}\n",
    "\n",
    "for c in set(classes):\n",
    "    if not os.path.isdir(exp + f'binned_reads/bin-{c}'):\n",
    "        os.mkdir(exp + f'binned_reads/bin-{c}')\n",
    "    bin_file[c] = open(exp + f'binned_reads/bin-{c}/reads.fasta', 'w+') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158636e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_class = {i:c for i,c in zip(idx, classes)}\n",
    "\n",
    "for n, record in tqdm(enumerate(SeqIO.parse(exp + 'reads.fasta', \"fasta\"))):\n",
    "    if n in idx_class:\n",
    "        bin_file[idx_class[n]].write(f'>{str(record.id)}\\n{str(record.seq)}\\n')\n",
    "\n",
    "for c in set(classes):\n",
    "    bin_file[c].close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
